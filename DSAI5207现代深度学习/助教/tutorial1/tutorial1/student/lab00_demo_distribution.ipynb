{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474412a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab00_demo_distribution.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Magic command to display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9f554",
   "metadata": {},
   "source": [
    "# DSAI5207: Deep Learning - Assignment 0 (Demo)\n",
    "\n",
    "**Welcome to DSAI5207!** üëã\n",
    "\n",
    "This is a **sample lab** designed to help you warm up for the semester. It serves two purposes:\n",
    "1.  **Refresher:** We review the core NumPy logic for **Linear Regression** and **Logistic Regression**.\n",
    "2.  **Workflow Demo:** This notebook demonstrates exactly how assignments in this course work, how to edit them, and how to submit them.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù How to Complete This Assignment\n",
    "\n",
    "This assignment is distributed as a **Jupyter Notebook (`.ipynb`)**. Your task is to implement the missing logic in specific code cells.\n",
    "\n",
    "### 1. Where to Write Code\n",
    "In every question, you will see a function definition. You must write your code **between** the `# BEGIN SOLUTION` and `# END SOLUTION` comments.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "def add(a, b):\n",
    "    # BEGIN SOLUTION\n",
    "    return a + b  <-- YOUR CODE GOES HERE\n",
    "    # END SOLUTION\n",
    "```\n",
    "\n",
    "### 2. Automatic Grading (Otter-Grader) ü¶¶\n",
    "We use an auto-grader. After every question, there is a cell labelled `Type: Test` or containing `assert` statements.\n",
    "* **Run these cells** to check your work immediately.\n",
    "* If the cell runs without error, your solution is likely correct!\n",
    "\n",
    "### 3. Important Rules ‚ö†Ô∏è\n",
    "* **DO NOT** modify the test cells.\n",
    "* **DO NOT** change the function names or arguments.\n",
    "* **DO NOT** delete any cells or change the structure of the notebook.\n",
    "* **ONLY** edit the code between the solution markers.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÆ Submission Instructions\n",
    "\n",
    "Once you have finished the assignment and all tests pass:\n",
    "\n",
    "1.  **Rename the file:** Change the filename to match the format `{STUDENT_ID}_A0.ipynb`.\n",
    "    * *Example:* If your ID is `24151913r`, your file should be named **`24151913r_A0.ipynb`**.\n",
    "2.  **Submit:** Upload your renamed `.ipynb` file to **Learn@PolyU**.\n",
    "\n",
    "**Let's get started building your first Neural Network layers!** üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978784e",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression (The Basics)\n",
    "\n",
    "Before we solve anything, we need to define **the model** (how we predict) and **the loss** (how we measure error).\n",
    "\n",
    "**Equation:**\n",
    "\n",
    "$$ \\hat{y} = XW + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdb5d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1: The Linear Forward Pass\n",
    "\n",
    "We have a matrix of inputs $X$ (shape `N, D`) and weights $W$ (shape `D, 1`) and bias $b$.\n",
    "\n",
    "**Task:** Implement `linear_forward(X, W, b)` using Matrix Multiplication (`@`) and Broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba04e9e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def linear_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Computes y = XW + b\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2b932",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d4ed8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2: How wrong are we? (MSE Loss)\n",
    "\n",
    "To train a model, we need to know how bad our predictions are. For regression, we use **Mean Squared Error**.\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_{pred}^{(i)} - y_{true}^{(i)})^2 $$\n",
    "\n",
    "**Task:** Implement `mse_loss(y_pred, y_true)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5048e03",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b9f29",
   "metadata": {},
   "source": [
    "# Part 2: Solving Linear Regression (Matrix Inversion)\n",
    "\n",
    "While modern Deep Learning uses Gradient Descent, Linear Regression has a beautiful \"Closed Form\" solution. We can find the **perfect** $W$ in one step using Matrix Algebra!\n",
    "\n",
    "This is known as the **Normal Equation**:\n",
    "\n",
    "$$ W = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "* $X^T$ is the transpose of $X$.\n",
    "* $^{-1}$ is the matrix inverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a03b94",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3: The Normal Equation\n",
    "\n",
    "**Task:** Implement `solve_linear_regression(X, y)` to calculate $W$ using the formula above.\n",
    "\n",
    "**Helpful NumPy functions:**\n",
    "* Matrix Multiplication: `A @ B` or `np.dot(A, B)`\n",
    "* Transpose: `X.T`\n",
    "* Inverse: `np.linalg.inv(A)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6afbe9",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def solve_linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    Computes W using the Normal Equation: W = (X^T X)^-1 X^T y\n",
    "    Arguments:\n",
    "        X: (N, D) matrix of inputs\n",
    "        y: (N, 1) vector of targets\n",
    "    Returns:\n",
    "        W: (D, 1) vector of weights\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d9b02",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìâ DEMO: The Perfect Line\n",
    "# Let's generate noisy data and use your Matrix Inversion solver to fit it.\n",
    "\n",
    "# 1. Generate Data (y = 4x + Noise)\n",
    "X_demo = 2 * np.random.rand(100, 1)\n",
    "y_demo = 4 + 3 * X_demo + np.random.randn(100, 1)\n",
    "\n",
    "# Add bias term (column of 1s) to X manually so we can use matrix math\n",
    "X_b = np.c_[np.ones((100, 1)), X_demo] \n",
    "\n",
    "# 2. Solve using YOUR function\n",
    "theta_best = solve_linear_regression(X_b, y_demo)\n",
    "\n",
    "print(f\"Model found: y = {theta_best[0][0]:.2f} + {theta_best[1][0]:.2f}x\")\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(X_demo, y_demo, color='blue', alpha=0.5, label='Data')\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "plt.plot(X_new, y_predict, \"r-\", linewidth=3, label=\"Normal Equation Fit\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear Regression via Matrix Inversion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c4e8f",
   "metadata": {},
   "source": [
    "# Part 3: Logistic Regression (Gradient Descent)\n",
    "\n",
    "For Logistic Regression (classifying Yes/No), there is **no** closed-form solution like the Normal Equation. We cannot just invert a matrix to find the answer.\n",
    "\n",
    "Instead, we use **Gradient Descent**. We calculate the error, find the slope (gradient), and nudge the weights in the opposite direction.\n",
    "\n",
    "**Key Formulas:**\n",
    "\n",
    "1.  **Prediction:** $\\hat{y} = \\sigma(XW + b)$\n",
    "2.  **Gradient (Slope):** $dW = \\frac{1}{N} X^T (\\hat{y} - y)$\n",
    "3.  **Update Rule:** $W_{new} = W_{old} - \\alpha \\cdot dW$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb174e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4: The Activation (Sigmoid)\n",
    "\n",
    "First, we need the sigmoid function to squash our outputs between 0 and 1.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Task:** Implement `sigmoid(x)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e8c5f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31c4b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 5: Compute Gradients\n",
    "\n",
    "To train the model, we first need to know which direction to move. We calculate the gradient (slope) of the loss with respect to weights ($dW$) and bias ($db$).\n",
    "\n",
    "$$ dW = \\frac{1}{N} X^T (\\hat{y} - y) $$\n",
    "$$ db = \\frac{1}{N} \\sum (\\hat{y} - y) $$\n",
    "\n",
    "**Task:** Implement `compute_gradients(X, y, W, b)`.\n",
    "1.  Compute forward pass prediction: `y_hat = sigmoid(X @ W + b)`.\n",
    "2.  Compute error: `error = y_hat - y`.\n",
    "3.  Compute `dW` and `db` using the formulas above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fcbbf",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, W, b):\n",
    "    N = X.shape[0]\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ce4b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31e877",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 6: Update Parameters\n",
    "\n",
    "Now that we have the gradients, we update the weights to move **opposite** to the error.\n",
    "\n",
    "$$ W_{new} = W - \\alpha \\cdot dW $$\n",
    "$$ b_{new} = b - \\alpha \\cdot db $$\n",
    "\n",
    "**Task:** Implement `update_parameters(W, b, dW, db, learning_rate)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea998b4c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters(W, b, dW, db, learning_rate):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ DEMO: Watching the Machine Learn\n",
    "# Let's run your new functions in a loop to see the line move!\n",
    "\n",
    "# 1. Data (2 clusters)\n",
    "X_cls = np.r_[np.random.randn(50, 2) - 2, np.random.randn(50, 2) + 2]\n",
    "y_cls = np.r_[np.zeros((50, 1)), np.ones((50, 1))]\n",
    "\n",
    "# 2. Initialize Weights randomly\n",
    "W = np.random.randn(2, 1)\n",
    "b = 0.0\n",
    "lr = 0.1\n",
    "\n",
    "print(\"Training started...\")\n",
    "losses = []\n",
    "\n",
    "# 3. Training Loop\n",
    "for i in range(100):\n",
    "    # Step 1: Get Gradients\n",
    "    dW, db = compute_gradients(X_cls, y_cls, W, b)\n",
    "    \n",
    "    # Step 2: Update Weights\n",
    "    W, b = update_parameters(W, b, dW, db, lr)\n",
    "    \n",
    "    # Calculate loss just for plotting\n",
    "    y_prob = sigmoid(X_cls @ W + b)\n",
    "    loss = -np.mean(y_cls * np.log(y_prob + 1e-9) + (1-y_cls) * np.log(1-y_prob + 1e-9))\n",
    "    losses.append(loss)\n",
    "\n",
    "print(f\"Final Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# 4. Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss Curve (Error going down)\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_cls[:, 0], X_cls[:, 1], c=y_cls.ravel(), cmap='coolwarm', edgecolors='k')\n",
    "# Plot decision boundary where Wx + b = 0 => x2 = -(w1*x1 + b)/w2\n",
    "x1_vals = np.linspace(-4, 4, 10)\n",
    "x2_vals = -(W[0]*x1_vals + b) / W[1]\n",
    "plt.plot(x1_vals, x2_vals, 'k--', linewidth=2, label='Decision Boundary')\n",
    "plt.title(\"Final Classification\")\n",
    "plt.legend()\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b90ab",
   "metadata": {},
   "source": [
    "# Part 4: Softmax (Multi-Class)\n",
    "\n",
    "Finally, what if we have 10 classes (like MNIST digits 0-9)? \n",
    "Logistic Regression gives us one probability ($p$). Softmax gives us a list of probabilities ($p_1, p_2, ..., p_N$) that sum to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2898f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 7: Softmax\n",
    "\n",
    "Convert raw scores (logits) into probabilities.\n",
    "\n",
    "$$ \\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum e^{x_j}} $$\n",
    "\n",
    "**Task:** Implement `softmax(x)`. \n",
    "*Tip: Subtract `np.max(x)` before exponentiating for numerical stability!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0bfff",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e726a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 8: Calculating Accuracy\n",
    "\n",
    "In classification, we output probabilities (e.g., `[0.1, 0.8, 0.1]`). \n",
    "To make a final decision, we pick the index with the **highest** probability using `np.argmax`.\n",
    "\n",
    "**Task:** Implement `get_accuracy(probs, labels)`.\n",
    "1. Convert `probs` to class predictions (index of max value).\n",
    "2. Compare with `labels`.\n",
    "3. Return the percentage (0.0 to 1.0) that match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fab99",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_accuracy(probs, labels):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dd1e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÜ DEMO: The Final Score\n",
    "# Let's see your Softmax and Accuracy in action.\n",
    "\n",
    "logits = np.array([-2.0, 1.5, 0.5]) # Raw scores\n",
    "probabilities = softmax(logits)     # Your function\n",
    "\n",
    "classes = ['Cat', 'Dog', 'Rabbit']\n",
    "prediction_index = np.argmax(probabilities)\n",
    "predicted_class = classes[prediction_index]\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probs:  {probabilities}\")\n",
    "print(f\"Robot says: 'It is a {predicted_class}!'\")\n",
    "\n",
    "plt.bar(classes, probabilities, color=['gray', 'green', 'gray'])\n",
    "plt.title(\"Model Confidence\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996e8f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdb730",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01d5c1",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAI5207 Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = np.array([[1, 2], [3, 4]])\n>>> W = np.array([[1], [1]])\n>>> b = 1\n>>> assert np.allclose(linear_forward(X, W, b), [[4], [8]])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test = np.array([[1, 2], [3, 4], [5, 6]])\n>>> y_test = np.array([[7], [17], [27]])\n>>> W_sol = solve_linear_regression(X_test, y_test)\n>>> assert np.allclose(W_sol, [[3], [2]]), 'Weights should be exactly [3, 2]'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_t = np.array([[1.0], [2.0]])\n>>> y_t = np.array([[0.0], [1.0]])\n>>> W_t = np.array([[0.5]])\n>>> b_t = 0.0\n>>> (dW, db) = compute_gradients(X_t, y_t, W_t, b_t)\n>>> assert np.isclose(dW, 0.042, atol=0.001)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> probs = np.array([[0.1, 0.9], [0.8, 0.2], [0.3, 0.7]])\n>>> labels = np.array([1, 0, 1])\n>>> assert get_accuracy(probs, labels) == 1.0\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
